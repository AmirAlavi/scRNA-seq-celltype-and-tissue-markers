{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "from joblib import dump, load\n",
    "from functools import partial\n",
    "import copy\n",
    "\n",
    "import anndata\n",
    "import scanpy as sc\n",
    "import diffxpy.api as de\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch import linalg as LA\n",
    "\n",
    "from skorch import NeuralNetClassifier\n",
    "from skorch.callbacks import Callback, EpochScoring, LRScheduler, PassthroughScoring, Checkpoint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, balanced_accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.utils.estimator_checks import check_estimator\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils.validation import check_array, check_is_fitted\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context(context=\"paper\")\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "\n",
    "import utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "torch.manual_seed(random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "This is a logistic regression classifier, where the output labels are actually a joint labeling, of both the cell type and the tissue type of each cell.\n",
    "\n",
    "We've called it a \"sequential\" classifier because in addition to the joint prediction above, we also combine the output from the joint nodes of the same cell type to get a cell type prediction score and also assess the cell type prediction alone. This is \"sequential\" because first we predict the (Tissue, Cell type) joint label, and then we use those predictions to also get the Cell type prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base model (Multinomial logistic regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultinomialLRModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.softmax(self.linear(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exclusive Lasso Penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclusive_lasso_penalty(W):\n",
    "    penalty = LA.norm(W, ord=1, dim=0)\n",
    "    penalty= LA.norm(penalty, ord=2)\n",
    "    return penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialMultiLabelClassifier(NeuralNetClassifier):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    ct_group_bounds:\n",
    "    ct_map:\n",
    "    secondary_criterion_weight:\n",
    "    lambda_clf:\n",
    "    regularization:\n",
    "    lambda_reg:\n",
    "    \"\"\"\n",
    "    def __init__(self, *args,\n",
    "                 ct_group_bounds=None,\n",
    "                 ct_map=None,\n",
    "                 secondary_criterion_weight=None,\n",
    "                 lambda_clf=1.,\n",
    "                 regularization=None,\n",
    "                 lambda_reg=0.01,\n",
    "                 **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.ct_group_bounds = ct_group_bounds\n",
    "        self.ct_map = ct_map\n",
    "        self.secondary_criterion_weight = secondary_criterion_weight\n",
    "        self.lambda_clf = lambda_clf\n",
    "        if regularization not in [None, 'l1', 'exclusive_l1']:\n",
    "            raise ValueError('Invalid regularization parameter.')\n",
    "        self.regularization = regularization\n",
    "        self.lambda_reg = lambda_reg\n",
    "\n",
    "    def get_ct_loss(self, y_pred, y_true):\n",
    "        y_true = torch.tensor(np.vectorize(self.ct_map.get)(y_true))\n",
    "\n",
    "        cumsum = y_pred.cumsum(1)\n",
    "        groupsum = cumsum[:, self.ct_group_bounds]\n",
    "        diff = torch.zeros_like(groupsum)\n",
    "        diff[:, 1:] = cumsum[:, self.ct_group_bounds[:-1]]\n",
    "        y_pred = groupsum - diff\n",
    "\n",
    "        return F.cross_entropy(y_pred, y_true, weight=self.secondary_criterion_weight)\n",
    "    \n",
    "    def get_loss(self, y_pred, y_true, X=None, training=False):\n",
    "        loss_multilabel = super().get_loss(y_pred, y_true, X=X, training=training)\n",
    "        loss_celltype = torch.tensor(0.)\n",
    "        if self.lambda_clf != 0.:\n",
    "            loss_celltype = self.get_ct_loss(y_pred, y_true)\n",
    "        loss_regularizer = torch.tensor(0.)\n",
    "        if self.regularization == 'exclusive_l1':\n",
    "            loss_regularizer = exclusive_lasso_penalty(self.module_.linear.weight)\n",
    "        elif self.regularization == 'l1':\n",
    "            loss_regularizer = self.module_.linear.weight.abs().sum()\n",
    "        total_loss = loss_multilabel + self.lambda_clf * loss_celltype + self.lambda_reg * loss_regularizer\n",
    "        \n",
    "        prefix = 'train' if training else 'valid'\n",
    "        self.history.record_batch(prefix + '_loss_ML', loss_multilabel.item())\n",
    "        self.history.record_batch(prefix + '_loss_CT', loss_celltype.item())\n",
    "        self.history.record_batch(prefix + '_loss_reg', loss_regularizer.item())\n",
    "        \n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Transforms\n",
    "Transformations that are applied to each input sample before being fed to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_transformer = FunctionTransformer(np.log1p, validate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLOAT_DTYPES = (np.float64, np.float32, np.float16)\n",
    "\n",
    "def check_non_neg(x):\n",
    "    if (x < 0.).any():\n",
    "        raise ValueError(\"Values in array must be non-negative\")\n",
    "    return x\n",
    "\n",
    "class CellTotalCountNormalizer(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self):\n",
    "        return\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = check_array(X, dtype=FLOAT_DTYPES)\n",
    "        X = check_non_neg(X)\n",
    "        cell_totals = X.sum(axis=1)\n",
    "        self.target_sum_ = np.median(cell_totals[cell_totals > 0])\n",
    "        self.n_features_ = X.shape[1]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        check_is_fitted(self)\n",
    "        X = check_array(X, dtype=FLOAT_DTYPES)\n",
    "        X = check_non_neg(X)\n",
    "        if X.shape[1] != self.n_features_:\n",
    "            raise ValueError(\"Number of features in transform is different from the number of features in fit\")\n",
    "        \n",
    "        cell_totals = X.sum(axis=1)\n",
    "        cell_totals += cell_totals == 0\n",
    "        cell_totals = cell_totals / self.target_sum_\n",
    "        \n",
    "        X = np.divide(X, cell_totals[:, None])\n",
    "        return X\n",
    "check_estimator(CellTotalCountNormalizer())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training utilities\n",
    "These include callback functions and scoring functions that track performance during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseWeightsThresholder(Callback):\n",
    "    def __init__(self, eps=1e-4):\n",
    "        self.eps = eps\n",
    "\n",
    "    def on_epoch_end(self, net, *args, **kwargs):\n",
    "         with torch.no_grad():\n",
    "            net.module_.linear.weight[net.module_.linear.weight.abs() < self.eps] = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ct_acc_score(net, X, y, ct_bounds, ct_map):\n",
    "    y_pred = net.forward(X)\n",
    "    y = torch.tensor(np.vectorize(ct_map.get)(y))\n",
    "\n",
    "    cumsum = y_pred.cumsum(1)\n",
    "    groupsum = cumsum[:, ct_bounds]\n",
    "    diff = torch.zeros_like(groupsum)\n",
    "    diff[:, 1:] = cumsum[:, ct_bounds[:-1]]\n",
    "    y_pred = groupsum - diff\n",
    "    y_pred = F.log_softmax(y_pred, dim=-1)\n",
    "\n",
    "    accuracy = (y_pred.argmax(dim=-1) == y).sum().float() / float(y.shape[0])\n",
    "    accuracy = accuracy.item()\n",
    "    return accuracy\n",
    "\n",
    "def _ct_balanced_acc_score(net, X, y, ct_bounds, ct_map):\n",
    "    y_pred = net.forward(X)\n",
    "    y = torch.tensor(np.vectorize(ct_map.get)(y))\n",
    "\n",
    "    cumsum = y_pred.cumsum(1)\n",
    "    groupsum = cumsum[:, ct_bounds]\n",
    "    diff = torch.zeros_like(groupsum)\n",
    "    diff[:, 1:] = cumsum[:, ct_bounds[:-1]]\n",
    "    y_pred = groupsum - diff\n",
    "    y_pred = F.log_softmax(y_pred, dim=-1)\n",
    "    y_pred = y_pred.argmax(dim=-1)\n",
    "    \n",
    "    return balanced_accuracy_score(y, y_pred)\n",
    "\n",
    "def get_class_weights(y_train, ct_map):\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    multilabel_class_weights = torch.from_numpy(class_weights).float()\n",
    "    \n",
    "    y_train_ct = np.vectorize(ct_map.get)(y_train)\n",
    "    celltype_class_weights = compute_class_weight('balanced', classes=np.unique(y_train_ct), y=y_train_ct)\n",
    "    celltype_class_weights = torch.from_numpy(celltype_class_weights).float()\n",
    "    return multilabel_class_weights, celltype_class_weights\n",
    "    \n",
    "\n",
    "def get_ct_scorers(ct_bounds, ct_map):   \n",
    "    ct_acc_score_fn = partial(_ct_acc_score, ct_bounds=ct_bounds, ct_map=ct_map)\n",
    "    ct_balanced_acc_score_fn = partial(_ct_balanced_acc_score, ct_bounds=ct_bounds, ct_map=ct_map)\n",
    "    return ct_acc_score_fn, ct_balanced_acc_score_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation & Plotting routines\n",
    "Inspect the fitted model performance metrics. Also use the model weights to determine marker genes for each (cell type, tissue type) phenotype, and inspect those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, X_train, y_train, X_test, y_test, le):\n",
    "    train_acc = model.score(X_train, y_train)\n",
    "    test_acc = model.score(X_test, y_test)\n",
    "    print(f\"train acc:         {train_acc}\")\n",
    "    print(f\"test acc:          {test_acc}\")\n",
    "    y_pred = model.predict(X_test)\n",
    "    test_bal_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "    print(f\"test balanced acc: {test_bal_acc}\")\n",
    "    print(\"classification report:\")\n",
    "    report = classification_report(y_test, y_pred, target_names=le.classes_)\n",
    "    print(report)\n",
    "    return test_bal_acc\n",
    "\n",
    "def consolidate_non_zero_features(coefs):\n",
    "    print(coefs.shape[1])\n",
    "    always_zero = np.ones(coefs.shape[1])\n",
    "    zeros = (coefs == 0).sum(axis=0)\n",
    "    always_zeros = zeros == coefs.shape[0]\n",
    "    print(f\"features that are always zero: {always_zeros.sum()}\")\n",
    "    return ~always_zeros\n",
    "\n",
    "def check_sparsity(lr_model, label_encoder, model_name=\"Softmax Regression Model\", figure_savefile=None):\n",
    "    # overall sparsity\n",
    "    n_weights = np.prod(lr_model.coef_.shape)\n",
    "    overall_sparsity = (n_weights - np.count_nonzero(lr_model.coef_)) / n_weights\n",
    "    print(f\"Overall sparsity: {overall_sparsity}\")\n",
    "    \n",
    "    sparsity = (lr_model.coef_.shape[1] - np.count_nonzero(lr_model.coef_, axis=1)) / lr_model.coef_.shape[1]\n",
    "    print(\"fraction of weights that are zero:\")\n",
    "    for class_name, sparsity in zip(label_encoder.inverse_transform(lr_model.classes_), sparsity):\n",
    "        print(f\"{class_name}: {sparsity}\")\n",
    "\n",
    "    non_zero_selector = consolidate_non_zero_features(lr_model.coef_)\n",
    "    n_always_zero = (~non_zero_selector).sum()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(20, 5))\n",
    "    sns.heatmap(lr_model.coef_[:, non_zero_selector], center=0, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set_yticks(np.arange(len(lr_model.classes_)))\n",
    "    # ... and label them with the respective list entries\n",
    "    ax.set_yticklabels(label_encoder.inverse_transform(lr_model.classes_), rotation=0)\n",
    "    ax.set_xlabel(\"Gene number\")\n",
    "    ax.set_ylabel(\"Class\")\n",
    "    if n_always_zero > 0:\n",
    "        ax.set_title(f'Fitted Coefficients for {model_name}\\n{n_always_zero} features are always zero')\n",
    "    else:\n",
    "        ax.set_title(f'Fitted Coefficients for {model_name}')\n",
    "    if figure_savefile is not None:\n",
    "        plt.savefig(f'{figure_savefile}.png')\n",
    "        plt.savefig(f'{figure_savefile}.pdf')\n",
    "    return overall_sparsity\n",
    "\n",
    "class SkorchLRAdapter:\n",
    "    def __init__(self, skorch_lr_model):\n",
    "        self.coef_ = skorch_lr_model.module_.linear.weight.detach().numpy()\n",
    "        self.classes_ = skorch_lr_model.classes_\n",
    "\n",
    "def find_exclusive_model_weights(weights, class_names, feature_symbols, k=20, verbose=False):\n",
    "    all_features_pos = []\n",
    "    all_features_pos_class_names = []\n",
    "    for class_value, class_name in zip(range(weights.shape[0]), class_names):\n",
    "        top_features_abs = (-np.abs(weights[class_value, :])).argsort()[:k]\n",
    "        top_features_pos = (-np.maximum(0., weights[class_value, :])).argsort()[:k]\n",
    "        top_features_neg = (np.minimum(0., weights[class_value, :])).argsort()[:k]\n",
    "\n",
    "        all_features_pos.append(set(feature_symbols.iloc[top_features_pos, 0]))\n",
    "        all_features_pos_class_names.append(class_name)\n",
    "        \n",
    "        tmp_df = feature_symbols.assign(weight = weights[class_value, :])\n",
    "        if verbose:\n",
    "            print(class_name)\n",
    "            print(f\"Top features (abs):\")\n",
    "            print(tmp_df.iloc[top_features_abs, :])\n",
    "            print()\n",
    "            print(f\"Top features (pos):\")\n",
    "            print(tmp_df.iloc[top_features_pos, :])\n",
    "            print()\n",
    "            print(f\"Top features (neg):\")\n",
    "            print(tmp_df.iloc[top_features_neg, :])\n",
    "            print()\n",
    "            print()\n",
    "            print()\n",
    "    \n",
    "    exclusive_results = {}\n",
    "    for i, class_label in enumerate(all_features_pos_class_names):\n",
    "        exclusive = all_features_pos[i].difference(set().union(*(all_features_pos[:i] + all_features_pos[i+1:])))\n",
    "        exclusive_results[class_label] = list(exclusive)\n",
    "        if verbose:\n",
    "            print(class_label)\n",
    "            print(exclusive)\n",
    "    \n",
    "    exclusive_lasso_result_df = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in exclusive_results.items() ]))\n",
    "    return exclusive_lasso_result_df\n",
    "\n",
    "def get_exclusive_model_weights_for_gsea(weights, class_names, feature_symbols, k=20, verbose=False):\n",
    "    all_features_pos = []\n",
    "    all_features_pos_scores = []\n",
    "    all_features_pos_class_names = []\n",
    "    for class_value, class_name in zip(range(weights.shape[0]), class_names):\n",
    "        top_features_abs = (-np.abs(weights[class_value, :])).argsort()[:k]\n",
    "        top_features_pos = (-np.maximum(0., weights[class_value, :])).argsort()[:k]\n",
    "        top_features_neg = (np.minimum(0., weights[class_value, :])).argsort()[:k]\n",
    "\n",
    "        tmp_df = feature_symbols.assign(weight = weights[class_value, :])\n",
    "        all_features_pos.append(set(feature_symbols.iloc[top_features_pos, 0]))\n",
    "        all_features_pos_scores.append(tmp_df.iloc[top_features_pos, :])\n",
    "        all_features_pos_class_names.append(class_name)\n",
    "        \n",
    "        if verbose:\n",
    "            print(class_name)\n",
    "            print(f\"Top features (abs):\")\n",
    "            print(tmp_df.iloc[top_features_abs, :])\n",
    "            print()\n",
    "            print(f\"Top features (pos):\")\n",
    "            print(tmp_df.iloc[top_features_pos, :])\n",
    "            print()\n",
    "            print(f\"Top features (neg):\")\n",
    "            print(tmp_df.iloc[top_features_neg, :])\n",
    "            print()\n",
    "            print()\n",
    "            print()\n",
    "    \n",
    "    exclusive_results = {}\n",
    "    exclusive_scores = {}\n",
    "    for i, class_label in enumerate(all_features_pos_class_names):\n",
    "        exclusive = all_features_pos[i].difference(set().union(*(all_features_pos[:i] + all_features_pos[i+1:])))\n",
    "        exclusive = list(exclusive)\n",
    "        exclusive_results[class_label] = exclusive\n",
    "        \n",
    "        score_df = all_features_pos_scores[i]\n",
    "        score_df = score_df[score_df['symbol'].isin(exclusive)]\n",
    "        exclusive_scores[class_label] = score_df\n",
    "        if verbose:\n",
    "            print(class_label)\n",
    "            print(exclusive)\n",
    "    \n",
    "    exclusive_lasso_result_df = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in exclusive_results.items() ]))\n",
    "    return exclusive_lasso_result_df, exclusive_scores\n",
    "\n",
    "def inspect_fitted_model(out_dir, regularization, fitted_model, adata, label_encoder, random_state):\n",
    "    model_reg_plot_string = \"\"\n",
    "    if regularization == 'l1':\n",
    "        model_reg_plot_string = \"L1 Regularized \"\n",
    "    elif regularization == 'exclusive_l1':\n",
    "        model_reg_plot_string = \"Exclusive L1 Regularized \"\n",
    "    model_name = f\"{model_reg_plot_string}Softmax Regression Model\"\n",
    "    sparsity = check_sparsity(SkorchLRAdapter(fitted_model['net']), label_encoder,\n",
    "                              model_name=model_name,\n",
    "                              figure_savefile=out_dir / \"coef\")\n",
    "    dump(fitted_model, out_dir / 'model.joblib')\n",
    " \n",
    "\n",
    "    weights = fitted_model['net'].module_.linear.weight.detach().numpy()\n",
    "    feature_symbols = adata.var[['symbol']]\n",
    "    exclusive_weights_df = find_exclusive_model_weights(weights, label_encoder.classes_, feature_symbols)\n",
    "    exclusive_weights_df.to_pickle(out_dir / 'exclusive_weights_table.pkl')\n",
    "    exclusive_weights_df.to_latex(out_dir / 'exclusive_weights_table.tex', index=False, na_rep=\"\", caption=\"\")\n",
    "\n",
    "    generate_model_gene_module_reports(adata, exclusive_weights_df, model_name, out_dir / \"model_gene_module_reports\", random_state)\n",
    "    \n",
    "    # De enrichment:\n",
    "    _, exclusive_scores = get_exclusive_model_weights_for_gsea(weights, label_encoder.classes_, feature_symbols)\n",
    "    enrich_folder = out_dir / \"enrich\"\n",
    "    enrich_folder.mkdir(exist_ok=True, parents=True)\n",
    "    for label, score_df in exclusive_scores.items():\n",
    "        save_file = enrich_folder / f\"{label}_GO_enrich.tex\"\n",
    "        utility.do_enrich(score_df['symbol'].tolist(), score_df['weight'], all_genes_info['symbol'], save_file)\n",
    "    return sparsity, exclusive_weights_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_module_score_violin(adata, genes, celltype, tissue, model_name, save_path):\n",
    "    # Cell identity embedding plots\n",
    "    combined_key = f\"{celltype},{tissue}\"\n",
    "    adata.obs[f\"is {combined_key}\"] = pd.Categorical((adata.obs_vector('celltype') == celltype) & (adata.obs_vector('tissue') == tissue))\n",
    "\n",
    "    # Gene-group score embedding plot\n",
    "    if np.array_equal(adata.var['symbol'], adata.var.index):\n",
    "        # index is symbols too, no need to use ensembl\n",
    "        score_genes = genes\n",
    "    else:\n",
    "        score_genes = [ adata.var.loc[adata.var['symbol'] == symb, 'ens'].iloc[0] for symb in genes]\n",
    "    \n",
    "    score_name = f'{combined_key} module score'\n",
    "    sc.tl.score_genes(adata, score_genes, score_name=score_name)\n",
    "\n",
    "    hits = adata.obs[adata.obs[f'is {combined_key}'] == True][f'{combined_key} module score']\n",
    "    others = adata.obs[adata.obs[f'is {combined_key}'] == False][f'{combined_key} module score']\n",
    "    hits_mean = hits.mean()\n",
    "    others_mean = others.mean()\n",
    "    hits_size = len(hits)\n",
    "    others_size = len(others)\n",
    "    \n",
    "    # Statistical testing\n",
    "    test_stat, pval = stats.ttest_ind(hits, others, equal_var=False, alternative='greater')\n",
    "    significance = \"\"\n",
    "    if pval <= 0.001:\n",
    "        significance = \"***\"\n",
    "    elif pval <= 0.01:\n",
    "        significance = \"**\"\n",
    "    elif pval <= 0.05:\n",
    "        significance = \"*\"\n",
    "    else:\n",
    "        significance = \"ns\"\n",
    "    significance = f\"{significance} $p$={pval:0.3e}\"\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    sns.violinplot(x=f'is {combined_key}', y=f'{combined_key} module score', data=adata.obs, order=[True, False], ax=ax)\n",
    "    fig.suptitle(f\"{model_name}\\n{combined_key} module score\", y=1.1)\n",
    "    subtitle = f\"Diff. of means (True - False): {hits_mean - others_mean:0.2f}\"\n",
    "    print(subtitle)\n",
    "    ax.set_title(subtitle)\n",
    "    xlabels = [item.get_text() for item in ax.get_xticklabels()]\n",
    "    xlabels[0] = f\"{xlabels[0]} (n={hits_size})\"\n",
    "    xlabels[1] = f\"{xlabels[1]} (n={others_size})\"\n",
    "    ax.set_xticklabels(xlabels)\n",
    "    ax.set_xlabel(f\"I{ax.get_xlabel()[1:]}?\")\n",
    "    # statistical annotation\n",
    "    pad = adata.obs[f'{combined_key} module score'].max() * 0.1\n",
    "    y_min, y_max = ax.get_ylim()\n",
    "    ax.set_ylim(y_min, y_max + (5*pad))\n",
    "    x1, x2 = 0, 1\n",
    "    \n",
    "    y, h, col = adata.obs[f'{combined_key} module score'].max() + (2*pad), pad, 'k'\n",
    "    ax.plot([x1, x1, x2, x2], [y, y+h, y+h, y], lw=1.0, c=col)\n",
    "    ax.text((x1+x2)*.5, y+h, significance, ha='center', va='bottom', color=col)\n",
    "    ax.text((x1+x2)*.5, y+h-(0.2*h), f\"$t$-stat={test_stat:0.3f}\", ha='center', va='top', color=col)\n",
    "    \n",
    "    plt.savefig(f\"{save_path}.png\", bbox_inches=\"tight\")\n",
    "    plt.savefig(f\"{save_path}.pdf\", bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "#     plt.show()\n",
    "\n",
    "def generate_model_gene_module_violin(adata, gene_df, model_name, folder, random_state, max_cells=50000):\n",
    "    if adata.shape[0] > max_cells:\n",
    "        print(f\"More than {max_cells} cells, subsampling...\")\n",
    "        adata = sc.pp.subsample(adata, n_obs=max_cells, random_state=random_state, copy=True)\n",
    "    if not isinstance(folder, Path):\n",
    "        folder = Path(folder)\n",
    "    folder.mkdir(parents=True, exist_ok=True)\n",
    "    for gene_module in gene_df.columns:\n",
    "        print(gene_module)\n",
    "        genes = gene_df[gene_module].dropna().tolist()\n",
    "        if len(genes) == 0:\n",
    "            print('no exlusive genes for this module')\n",
    "            continue\n",
    "        print(f\"genes: {genes}\")\n",
    "        celltype, tissue = gene_module.split(',')\n",
    "        report_path = folder / f\"{model_name} {celltype}_{tissue}\"\n",
    "        get_module_score_violin(adata, genes, celltype, tissue, model_name, report_path)  \n",
    "\n",
    "def gene_expression_report(adata, genes, celltype, tissue, model_name, save_path):\n",
    "    if 'X_umap' not in adata.obsm:\n",
    "        print(\"computing umap...\")\n",
    "        sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "        sc.pp.log1p(adata)\n",
    "        adata.raw = adata\n",
    "        sc.pp.scale(adata)\n",
    "        sc.tl.pca(adata)\n",
    "        sc.pp.neighbors(adata, n_pcs=50)\n",
    "        sc.tl.umap(adata)\n",
    "        print(\"done.\")\n",
    "    \n",
    "    n_plots = 4 + len(genes)\n",
    "    n_cols = 5\n",
    "    n_rows = int(np.ceil(n_plots / n_cols))\n",
    "    width = 10\n",
    "    height = 2 * n_rows\n",
    "    fig, all_axs = plt.subplots(n_rows, n_cols, figsize=(width, height), gridspec_kw={'hspace': 0.3, 'wspace': 0.3})\n",
    "\n",
    "    # Cell identity embedding plots\n",
    "    combined_key = f\"{celltype},{tissue}\"\n",
    "    adata.obs[f\"is_{combined_key}\"] = pd.Categorical((adata.obs_vector('celltype') == celltype) & (adata.obs_vector('tissue') == tissue))\n",
    "    adata.uns[f'is_{combined_key}_colors'] = ['grey', 'red']\n",
    "\n",
    "    adata.obs[f'is_{celltype}'] = pd.Categorical(adata.obs_vector('celltype') == celltype)\n",
    "    adata.uns[f'is_{celltype}_colors'] = ['grey', 'red']\n",
    "    \n",
    "    adata.obs[f'is_{tissue}'] = pd.Categorical(adata.obs_vector('tissue') == tissue)\n",
    "    adata.uns[f'is_{tissue}_colors'] = ['grey', 'red']\n",
    "\n",
    "    sc.pl.umap(adata, color=f'is_{combined_key}', title=[combined_key], show=False, ax=all_axs[0, 0])\n",
    "    sc.pl.umap(adata, color=f'is_{celltype}', title=[celltype], show=False, ax=all_axs[0, 1])\n",
    "    sc.pl.umap(adata, color=f'is_{tissue}', title=[tissue], show=False, ax=all_axs[0, 2])\n",
    "    for ax in all_axs[0, :3]:\n",
    "        ax.get_legend().remove()\n",
    "\n",
    "    # Gene-group score embedding plot\n",
    "    if np.array_equal(adata.var['symbol'], adata.var.index):\n",
    "        # index is symbols too, no need to use ensembl\n",
    "        score_genes = genes\n",
    "    else:\n",
    "        score_genes = [ adata.var.loc[adata.var['symbol'] == symb, 'ens'].iloc[0] for symb in genes]\n",
    "    \n",
    "    score_name = 'Module score'\n",
    "    sc.tl.score_genes(adata, score_genes, score_name=score_name)\n",
    "    sc.pl.umap(adata, color=score_name, show=False, ax=all_axs[0, 3], color_map=\"viridis\")\n",
    "    \n",
    "    \n",
    "    # All gene embedding plots\n",
    "    for i, gene in enumerate(genes):\n",
    "        row_idx = ((i + 4) // n_cols)\n",
    "        col_idx = (i + 4) % n_cols\n",
    "        sc.pl.umap(adata, color=gene, gene_symbols='symbol', show=False, ax=all_axs[row_idx, col_idx], color_map=\"viridis\")\n",
    "    \n",
    "    for i in range(n_cols):\n",
    "        for j in range(n_rows - 1):\n",
    "            all_axs[j, i].set_xlabel(None)\n",
    "    for i in range(1, n_cols):\n",
    "        for j in range(n_rows):\n",
    "            all_axs[j, i].set_ylabel(None)\n",
    "    for i in range(4 + len(genes), n_cols*n_rows):\n",
    "        row_idx = i // n_cols\n",
    "        col_idx = i % n_cols\n",
    "        fig.delaxes(all_axs[row_idx][col_idx])\n",
    "#     fig.suptitle(f\"{combined_key} gene module report for {model_name}\", y=0.92, fontsize='x-large')\n",
    "    plt.savefig(f\"{save_path}_150dpi.png\", dpi=150)\n",
    "    plt.savefig(f\"{save_path}_200dpi.png\", dpi=200)\n",
    "#     plt.savefig(f\"{save_path}_300dpi.png\", dpi=300)\n",
    "#     plt.savefig(f\"{save_path}.pdf\")\n",
    "    plt.close()\n",
    "\n",
    "def generate_model_gene_module_reports(adata, gene_df, model_name, folder, random_state, max_cells=50000):\n",
    "    if adata.shape[0] > max_cells:\n",
    "        print(f\"More than {max_cells} cells, subsampling...\")\n",
    "        adata = sc.pp.subsample(adata, n_obs=max_cells, random_state=random_state, copy=True)\n",
    "    if not isinstance(folder, Path):\n",
    "        folder = Path(folder)\n",
    "    folder.mkdir(parents=True, exist_ok=True)\n",
    "    for gene_module in gene_df.columns:\n",
    "        print(gene_module)\n",
    "        genes = gene_df[gene_module].dropna().tolist()\n",
    "        if len(genes) == 0:\n",
    "            print('no exlusive genes for this module')\n",
    "            continue\n",
    "        print(f\"genes: {genes}\")\n",
    "        celltype, tissue = gene_module.split(',')\n",
    "        report_path = folder / f\"{celltype}_{tissue}\"\n",
    "        gene_expression_report(adata, genes, celltype, tissue, model_name, report_path)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis pipeline\n",
    "The \"main\" or \"driver\" code that puts all the above together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_checkpointed_model(X_train, y_train, X_test, y_test, le, checkpoint_dir, regularization, lambda_reg,\n",
    "                           ct_bounds, ct_map, multilabel_class_weights, balanced_acc, ct_acc, ct_balanced_acc,\n",
    "                           ct_acc_score_fn, ct_balanced_acc_score_fn, celltype_class_weights,\n",
    "                           learning_rate, lr_plateau_factor=0.5, optimizer=torch.optim.Adam, epochs=200):\n",
    "    cp = Checkpoint(dirname=checkpoint_dir)\n",
    "    net = SequentialMultiLabelClassifier(MultinomialLRModel,\n",
    "                                         module__input_size=X_train.shape[1],\n",
    "                                         module__output_size=len(le.classes_),\n",
    "                                         regularization=regularization,\n",
    "                                         ct_group_bounds=ct_bounds,\n",
    "                                         ct_map=ct_map,\n",
    "                                         criterion__weight=multilabel_class_weights,\n",
    "                                         max_epochs=epochs,\n",
    "                                         optimizer=optimizer,\n",
    "                                         lr=learning_rate,\n",
    "                                         iterator_train__shuffle=True,\n",
    "                                         callbacks=[('balanced_acc', balanced_acc),\n",
    "                                                    ('celltype_acc', ct_acc),\n",
    "                                                    ('celltype_balanced_acc', ct_balanced_acc),\n",
    "                                                    ('lr_scheduler', LRScheduler(policy=torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "                                                                                 factor=lr_plateau_factor, verbose=True)),\n",
    "                                                    ('train_loss_ML', PassthroughScoring(name='train_loss_ML', on_train=True)),\n",
    "                                                    ('train_loss_CT', PassthroughScoring(name='train_loss_CT', on_train=True)),\n",
    "                                                    ('sparse_thresholder', SparseWeightsThresholder()),\n",
    "                                                    ('checkpoint', cp),\n",
    "                                                   ],\n",
    "                                         lambda_clf=0.,\n",
    "                                         secondary_criterion_weight=celltype_class_weights,\n",
    "                                         lambda_reg=lambda_reg) #\n",
    "\n",
    "    pipe = Pipeline([\n",
    "            ('normalize_total', CellTotalCountNormalizer()),\n",
    "            ('log', log_transformer),\n",
    "            ('scale', StandardScaler()),\n",
    "            ('net', net),\n",
    "        ])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    pipe['net'].initialize()\n",
    "    pipe['net'].load_params(checkpoint=cp)\n",
    "    test_balanced_acc = eval_model(pipe, X_train, y_train, X_test, y_test, le)\n",
    "    test_ct_acc = ct_acc_score_fn(net, np.asarray(X_test), y_test)\n",
    "    print(f\"test CT acc: {test_ct_acc}\")\n",
    "    test_ct_balanced_acc = ct_balanced_acc_score_fn(net, np.asarray(X_test), y_test)\n",
    "    print(f\"test CT balanced acc: {test_ct_balanced_acc}\")\n",
    "    print()\n",
    "    return pipe, test_balanced_acc \n",
    "\n",
    "def build_ct_map(ct_group_bounds):\n",
    "    ct_map = {}\n",
    "    bound_idx = 0\n",
    "    for i in range(ct_group_bounds[-1] + 1):\n",
    "        if i > ct_group_bounds[bound_idx]:\n",
    "            bound_idx += 1\n",
    "        ct_map[i] = bound_idx\n",
    "    return ct_map\n",
    "\n",
    "def run_analysis_for_regularization(regularization, X_train, X_test, y_train, y_test, le, adata, all_genes_info, result_folder, random_state, cell_types_list=None, n_jobs=-1):\n",
    "    # if not isinstance(result_folder, Path):\n",
    "    #     result_folder = Path(result_folder)\n",
    "    # best_settings_dir = result_folder / 'best_settings'\n",
    "    # best_settings_dir.mkdir(parents=True, exist_ok=True)\n",
    "    # extra_pen_dir = result_folder / 'extra_penalty'\n",
    "    # extra_pen_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Set up evaluation metrics\n",
    "    ct_bounds = []\n",
    "    ct_classes = [s.split(',')[0] for s in le.classes_]\n",
    "    if len(np.unique(ct_classes)) > 1:\n",
    "        cur_ct = ct_classes[0]\n",
    "        for i in range(len(ct_classes)):\n",
    "            if ct_classes[i] != cur_ct:\n",
    "                ct_bounds.append(i - 1)\n",
    "                cur_ct = ct_classes[i] \n",
    "    ct_bounds.append(len(ct_classes) - 1)\n",
    "    ct_map = build_ct_map(ct_bounds)\n",
    "\n",
    "    ct_acc_score_fn, ct_balanced_acc_score_fn = get_ct_scorers(ct_bounds, ct_map)\n",
    "    multilabel_class_weights, celltype_class_weights = get_class_weights(y_train, ct_map)\n",
    "    # Balanced joint accuracy\n",
    "    balanced_acc = EpochScoring(name='balanced_acc', scoring='balanced_accuracy', lower_is_better=False)\n",
    "    # Cell type accuracy\n",
    "    ct_acc = EpochScoring(name='CT_acc', scoring=ct_acc_score_fn, lower_is_better=False)\n",
    "    # Balanced cell type accuracy\n",
    "    ct_balanced_acc = EpochScoring(name='CT_balanced_acc', scoring=ct_balanced_acc_score_fn, lower_is_better=False)\n",
    "\n",
    "    # Run a quick cross validated grid-search to get the best hyperparemeters\n",
    "    param_grid = {\n",
    "        'net__max_epochs': [10, 20, 100],\n",
    "        'net__lr': [0.01, 0.001],\n",
    "        'net__optimizer': [torch.optim.Adam],\n",
    "        'net__callbacks__lr_scheduler__factor': [0.5],\n",
    "        'net__lambda_clf': [0.],\n",
    "        'net__lambda_reg': [0., 0.001, 0.01, 0.1],\n",
    "        'net__regularization': [regularization]\n",
    "    }\n",
    "    # #     # Just for debugging\n",
    "    # param_grid = {\n",
    "    #     'net__max_epochs': [1],\n",
    "    #     'net__lr': [0.01],\n",
    "    #     'net__optimizer': [torch.optim.Adam],\n",
    "    #     'net__callbacks__lr_scheduler__factor': [0.5],\n",
    "    #     'net__lambda_clf': [0.],\n",
    "    #     'net__lambda_reg': [0.01],\n",
    "    #     'net__regularization': [regularization]\n",
    "    # }\n",
    "\n",
    "    net = SequentialMultiLabelClassifier(MultinomialLRModel,\n",
    "                                         module__input_size=X_train.shape[1],\n",
    "                                         module__output_size=len(le.classes_),\n",
    "                                         ct_group_bounds=ct_bounds,\n",
    "                                         ct_map=ct_map,\n",
    "                                         criterion__weight=multilabel_class_weights,\n",
    "                                         iterator_train__shuffle=True,\n",
    "                                         callbacks=[('balanced_acc', balanced_acc),\n",
    "                                                    ('celltype_acc', ct_acc),\n",
    "                                                    ('celltype_balanced_acc', ct_balanced_acc),\n",
    "                                                    ('lr_scheduler', LRScheduler(policy=torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "                                                                                 factor=0.5, verbose=True)),\n",
    "                                                    ('train_loss_ML', PassthroughScoring(name='train_loss_ML', on_train=True)),\n",
    "                                                    ('train_loss_CT', PassthroughScoring(name='train_loss_CT', on_train=True)),\n",
    "                                                    ('sparse_thresholder', SparseWeightsThresholder()),\n",
    "                                                   ],\n",
    "                                         secondary_criterion_weight=celltype_class_weights)\n",
    "    pipe = Pipeline([\n",
    "            ('normalize_total', CellTotalCountNormalizer()),\n",
    "            ('log', log_transformer),\n",
    "            ('scale', StandardScaler()),\n",
    "            ('net', net),\n",
    "        ])\n",
    "\n",
    "    gs = GridSearchCV(pipe, param_grid, refit=True, cv=3, scoring='balanced_accuracy', verbose=4, n_jobs=n_jobs)\n",
    "    gs.fit(X_train, y_train)\n",
    "    dump(gs, result_folder / 'quick_gridsearch.joblib')\n",
    "    with open(result_folder / \"quick_gridsearch_best_params.txt\", 'w') as text_file:\n",
    "        text_file.write(str(gs.best_params_))\n",
    "    print(gs.best_params_)\n",
    "\n",
    "    results = {}\n",
    "    # Fit a checkpointed model with the best parameters:\n",
    "    pipe, acc = fit_checkpointed_model(X_train, y_train, X_test, y_test, le, result_folder / 'checkpoints', regularization, gs.best_params_['net__lambda_reg'],\n",
    "                                                 ct_bounds, ct_map, multilabel_class_weights, balanced_acc, ct_acc, ct_balanced_acc, ct_acc_score_fn, ct_balanced_acc_score_fn, celltype_class_weights,\n",
    "                                                 gs.best_params_['net__lr'], gs.best_params_['net__callbacks__lr_scheduler__factor'], gs.best_params_['net__optimizer'], epochs=200)\n",
    "    results['acc'] = acc\n",
    "    sparsity, exclusive_weights_df = inspect_fitted_model(result_folder, regularization, pipe, adata, le, random_state)\n",
    "    results['sparsity'] = sparsity\n",
    "    results['exclusive_weights_df'] = exclusive_weights_df\n",
    "    \n",
    "    with open(result_folder / 'results_dict.pickle', 'wb') as f:\n",
    "        pickle.dump(results, f)\n",
    "    return pipe, results\n",
    "\n",
    "def get_X_y_data(adata, random_state=None, min_label_count=100):\n",
    "    adata.obs['celltype_and_tissue'] = adata.obs[['celltype', 'tissue']].agg(','.join, axis=1)\n",
    "    \n",
    "    label_counts = adata.obs['celltype_and_tissue'].value_counts()\n",
    "    labels_to_keep = label_counts[label_counts >= min_label_count].index\n",
    "    adata = adata[adata.obs['celltype_and_tissue'].isin(labels_to_keep)]\n",
    "    \n",
    "    X = adata.X\n",
    "    y = adata.obs['celltype_and_tissue']\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=random_state)\n",
    "    return le, X_train, X_test, y_train, y_test\n",
    "\n",
    "# Run analysis pipeline (usually, called once per dataset)\n",
    "def run_analysis_pipeline(adata, all_genes_info, result_folder, random_state, cell_types_list=None, n_jobs=-1, min_label_count=100, de_markers_df=None):\n",
    "    if not isinstance(result_folder, Path):\n",
    "        result_folder = Path(result_folder)\n",
    "        \n",
    "    l1_dir = result_folder / 'L1'\n",
    "    l1_dir.mkdir(parents=True, exist_ok=True)\n",
    "    ex_l1_dir = result_folder / 'Exclusive_L1'\n",
    "    ex_l1_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    fig_folder = result_folder / \"regularization_comparison_figs\"\n",
    "    fig_folder.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Set up data for ML model consumption\n",
    "    if cell_types_list is None:\n",
    "        cell_types_list = adata.obs[\"celltype\"].unique()\n",
    "    le, X_train, X_test, y_train, y_test = get_X_y_data(adata[adata.obs[\"celltype\"].isin(cell_types_list)],\n",
    "                                                        random_state=random_state, min_label_count=min_label_count)\n",
    "    if isinstance(X_train, scipy.sparse.spmatrix):\n",
    "        X_train, X_test = X_train.todense(), X_test.todense()\n",
    "    X_train, X_test = np.asarray(X_train), np.asarray(X_test)\n",
    "    \n",
    "    # L1\n",
    "    l1_model, l1_results = run_analysis_for_regularization('l1', X_train, X_test, y_train, y_test, le, adata, all_genes_info, l1_dir, random_state, cell_types_list, n_jobs)\n",
    "    # Exclusive L1\n",
    "    ex_l1_model, ex_l1_results = run_analysis_for_regularization('exclusive_l1', X_train, X_test, y_train, y_test, le, adata, all_genes_info, ex_l1_dir, random_state, cell_types_list, n_jobs)\n",
    "    \n",
    "    # Compile results\n",
    "    res_df = {\n",
    "        'Regularization': ['Exclusive L1', 'L1'],\n",
    "        'Test accuracy': [ex_l1_results['acc'],\n",
    "                          l1_results['acc'],\n",
    "                         ],\n",
    "        'Sparsity': [ex_l1_results['sparsity'],\n",
    "                     l1_results['sparsity'],\n",
    "                    ],\n",
    "    }\n",
    "    res_df = pd.DataFrame(res_df)\n",
    "    \n",
    "    # Test accuracy\n",
    "    with sns.axes_style(\"whitegrid\"):\n",
    "        fig, ax = plt.subplots(figsize=(4, 1.5))\n",
    "        sns.barplot(y=\"Regularization\", x=\"Test accuracy\", data=res_df, order=['L1', 'Exclusive L1'], orient='h', ax=ax)\n",
    "        ax.set_title('Model Accuracy')\n",
    "        ax.set_xlim((0, 1))\n",
    "        filename = \"test_accuracy\"\n",
    "        plt.savefig(fig_folder / f\"{filename}.png\", bbox_inches=\"tight\")\n",
    "        plt.savefig(fig_folder / f\"{filename}.pdf\", bbox_inches=\"tight\")\n",
    "        plt.show()\n",
    "    \n",
    "    # Sparsity\n",
    "    with sns.axes_style(\"whitegrid\"):\n",
    "        fig, ax = plt.subplots(figsize=(4, 1.5))\n",
    "        sns.barplot(y=\"Regularization\", x=\"Sparsity\", data=res_df, order=['L1', 'Exclusive L1'], orient='h', ax=ax)\n",
    "        ax.set_title('Model Sparsity')\n",
    "        filename = \"model_sparsity\"\n",
    "        plt.savefig(fig_folder / f\"{filename}.png\", bbox_inches=\"tight\")\n",
    "        plt.savefig(fig_folder / f\"{filename}.pdf\", bbox_inches=\"tight\")\n",
    "        plt.show()\n",
    "    \n",
    "    # Marker yeilds\n",
    "    marker_rec_arr = [\n",
    "        l1_results['exclusive_weights_df'].count().append(pd.Series({'Model': 'L1'})),\n",
    "        ex_l1_results['exclusive_weights_df'].count().append(pd.Series({'Model': 'Exclusive L1'})),\n",
    "    ]\n",
    "    hue_order = ['L1', 'Exclusive L1']\n",
    "    if de_markers_df is not None:\n",
    "        marker_rec_arr.append(de_markers_df.count().append(pd.Series({'Model': 'DE'})))\n",
    "        hue_order.insert(0, 'DE')\n",
    "    marker_df = pd.DataFrame.from_records(marker_rec_arr)\n",
    "    marker_df = pd.melt(marker_df, id_vars=['Model'], var_name = \"Phenotype\", value_name=\"Exclusive marker count\")\n",
    "    marker_df['Phenotype'] = marker_df['Phenotype'].str.replace('_',' ')\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    sns.barplot(x=\"Phenotype\",\n",
    "                     y=\"Exclusive marker count\",\n",
    "                     hue=\"Model\",\n",
    "                     data=marker_df,\n",
    "                     hue_order=hue_order,\n",
    "                ax=ax,\n",
    "                     )\n",
    "    ax.set_title('Marker Exclusivity')\n",
    "    ax.set_xticklabels(ax.get_xticklabels(),rotation=90)\n",
    "    # ax.legend(bbox_to_anchor=(1.04,0.5), loc=\"center left\")\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1.01, -0.35))\n",
    "    filename = \"marker_exclusivity\"\n",
    "    plt.savefig(fig_folder / f\"{filename}.png\", bbox_inches=\"tight\")\n",
    "    plt.savefig(fig_folder / f\"{filename}.pdf\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(\"DONE.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_folder = Path(\"results/human\")\n",
    "result_folder.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = utility.load_all_human_data(\"data/hubmap/\", \"data/van_der_Wijst_PBMC/count_matrices_per_lane/\")\n",
    "all_genes_info = adata.var\n",
    "print(f\"Initial data shape: {adata.shape}\")\n",
    "old_n_genes = adata.shape[1]\n",
    "adata, hvg_genes = utility.filter_hvg(adata)\n",
    "print(f\"After hvg shape: {adata.shape}\")\n",
    "print(f\"\\tRemoved {old_n_genes - adata.shape[1]} genes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell type subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_types_list = ['T Cell', 'Fibroblast', 'B Cell', 'NK Cell']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Differential Expression for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_de = adata[adata.obs[\"celltype\"].isin(cell_types_list)]\n",
    "sc.pp.normalize_total(adata_de, target_sum=1e4)\n",
    "sc.pp.log1p(adata_de)\n",
    "sc.pp.highly_variable_genes(adata_de, min_mean=0.0125, max_mean=3, min_disp=0.5)\n",
    "adata_de.obs['celltype_and_tissue'] = adata_de.obs[['celltype', 'tissue']].agg(','.join, axis=1)\n",
    "label_counts = adata_de.obs['celltype_and_tissue'].value_counts()\n",
    "min_label_count = 100\n",
    "labels_to_keep = label_counts[label_counts >= min_label_count].index\n",
    "adata_de = adata_de[adata_de.obs['celltype_and_tissue'].isin(labels_to_keep)]\n",
    "adata_de.raw = adata_de\n",
    "sc.tl.rank_genes_groups(adata_de, 'celltype_and_tissue', method='wilcoxon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phenotypes = adata_de.obs['celltype_and_tissue'].unique()\n",
    "de_top_genes = []\n",
    "for i, pheno in enumerate(phenotypes):\n",
    "    dedf = sc.get.rank_genes_groups_df(adata_de, group=pheno, gene_symbols='symbol', log2fc_min=0.)\n",
    "    top_genes = dedf['symbol'][:20].tolist()\n",
    "    de_top_genes.append(set(top_genes))\n",
    "\n",
    "exclusive_results = {}\n",
    "for i, class_label in enumerate(phenotypes):\n",
    "    exclusive = de_top_genes[i].difference(set().union(*(de_top_genes[:i] + de_top_genes[i+1:])))\n",
    "    exclusive_results[class_label] = list(exclusive)\n",
    "    print(class_label)\n",
    "    print(exclusive)\n",
    "\n",
    "de_exclusive_df = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in exclusive_results.items() ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Exclusive L1 classifier framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_analysis_pipeline(adata, all_genes_info, result_folder, random_state, cell_types_list=cell_types_list, n_jobs=6, de_markers_df=de_exclusive_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mouse Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_folder = Path(\"results/mouse\")\n",
    "result_folder.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = utility.load_tabula_muris_data(\"data/tabula_muris\", ['Lung', 'Limb_Muscle', 'Fat', 'Spleen'])\n",
    "all_genes_info = adata.var\n",
    "print(f\"Initial data shape: {adata.shape}\")\n",
    "old_n_genes = adata.shape[1]\n",
    "adata, hvg_genes = utility.filter_hvg(adata)\n",
    "print(f\"After hvg shape: {adata.shape}\")\n",
    "print(f\"\\tRemoved {old_n_genes - adata.shape[1]} genes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell type subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_types_list = ['B cell', 'T cell', 'natural killer cell']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Differential Expression for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_de = adata[adata.obs[\"celltype\"].isin(cell_types_list)]\n",
    "sc.pp.normalize_total(adata_de, target_sum=1e4)\n",
    "sc.pp.log1p(adata_de)\n",
    "sc.pp.highly_variable_genes(adata_de, min_mean=0.0125, max_mean=3, min_disp=0.5)\n",
    "adata_de.obs['celltype_and_tissue'] = adata_de.obs[['celltype', 'tissue']].agg(','.join, axis=1)\n",
    "label_counts = adata_de.obs['celltype_and_tissue'].value_counts()\n",
    "min_label_count = 100\n",
    "labels_to_keep = label_counts[label_counts >= min_label_count].index\n",
    "adata_de = adata_de[adata_de.obs['celltype_and_tissue'].isin(labels_to_keep)]\n",
    "adata_de.raw = adata_de\n",
    "sc.tl.rank_genes_groups(adata_de, 'celltype_and_tissue', method='wilcoxon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phenotypes = adata_de.obs['celltype_and_tissue'].unique()\n",
    "de_top_genes = []\n",
    "for i, pheno in enumerate(phenotypes):\n",
    "    dedf = sc.get.rank_genes_groups_df(adata_de, group=pheno, gene_symbols='symbol', log2fc_min=0.)\n",
    "    top_genes = dedf['symbol'][:20].tolist()\n",
    "    de_top_genes.append(set(top_genes))\n",
    "\n",
    "exclusive_results = {}\n",
    "for i, class_label in enumerate(phenotypes):\n",
    "    exclusive = de_top_genes[i].difference(set().union(*(de_top_genes[:i] + de_top_genes[i+1:])))\n",
    "    exclusive_results[class_label] = list(exclusive)\n",
    "    print(class_label)\n",
    "    print(exclusive)\n",
    "\n",
    "de_exclusive_df = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in exclusive_results.items() ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Exclusive L1 classifier framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_analysis_pipeline(adata, all_genes_info, result_folder, random_state, cell_types_list=cell_types_list, n_jobs=6, de_markers_df=de_exclusive_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b784c6f3bb033743fe3d2694a32bf73864e6f1a767f2aac5d6559c8391c56a09"
  },
  "kernelspec": {
   "display_name": "Python 3.7.8 64-bit ('hubmap': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
